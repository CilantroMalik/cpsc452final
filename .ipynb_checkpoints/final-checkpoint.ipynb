{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b620c-9456-41b4-9e88-f11d8090f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter\n",
    "!pip install torch-geometric\n",
    "!pip install plotnine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e57e8cfe-3031-4e8b-9f4a-0a8eac7ea6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, InMemoryDataset, Dataset\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, DMoNPooling\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from plotnine import ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308170d3-8192-4cd2-a7dc-16e32ff60105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "metacells = pd.read_csv(\"./oligo-SCZ-metacellExpr.csv\", index_col=0)\n",
    "metadata = pd.read_csv(\"./oligo-SCZ-meta.csv\", index_col=0)\n",
    "tom = pd.read_csv(\"./oligo-SCZ-tom.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3688d75-b0e4-4f0c-afc2-9c0976efa9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Oligo#CON1_2             Control\n",
       "Oligo#CON10_5            Control\n",
       "Oligo#CON10_55           Control\n",
       "Oligo#CON10_105          Control\n",
       "Oligo#CON11_41           Control\n",
       "                       ...      \n",
       "Oligo#SZ15_10      Schizophrenia\n",
       "Oligo#SZ16_47      Schizophrenia\n",
       "Oligo#SZ16_97      Schizophrenia\n",
       "Oligo#SZ16_147     Schizophrenia\n",
       "Oligo#SZ18_1       Schizophrenia\n",
       "Name: disorder, Length: 100, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.iloc[1:5000:50,:][\"disorder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcf3c1a-3706-4ec2-bd91-c101d2f22c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 1.28492008, ..., 3.60950374, 1.82917698,\n",
       "       0.83603014])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metacells.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "80ce2079-8a74-4111-a1c1-052721ff72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "TOM_THRESHOLD = 0.03  # value below which we zero out the similarity that will be used as attention priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8734aa97-8f22-470f-82d9-b36b4a8dc813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6679/6679 [04:56<00:00, 22.54it/s] \n"
     ]
    }
   ],
   "source": [
    "# create graph dataset\n",
    "edges = []\n",
    "for i in tqdm(range(len(tom.columns))):\n",
    "    for j in range(i):\n",
    "        if tom.iloc[i,j] > TOM_THRESHOLD:\n",
    "            edges.extend([[i,j],[j,i]])\n",
    "#edges = torch.tensor(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df5d3d7-e95b-4f52-b72d-8a094c40193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOT USING CURRENTLY\n",
    "\n",
    "# class GeneGraphDataset(InMemoryDataset):\n",
    "#     def __init__(self, root, edge_index, expr_mat, meta, transform=None, pre_transform=None):\n",
    "#         self.edge_index = edge_index\n",
    "#         self.num_graphs = expr_mat.shape[1]\n",
    "#         self.expr_mat = expr_mat\n",
    "#         self.num_classes = 2\n",
    "#         self.y = meta[\"disorder\"]\n",
    "#         super().__init__(root, transform, pre_transform)\n",
    "#         self.load(\"./graphData/processed/combined.pt\")\n",
    "\n",
    "#     @property\n",
    "#     def raw_file_names(self):\n",
    "#         pass\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self):\n",
    "#         return [f\"./graphData/processed/{self.expr_mat.columns[i]}-graph.pt\" for i in range(self.num_graphs)]\n",
    "\n",
    "#     def process(self):\n",
    "#         data_list = []\n",
    "#         for i in tqdm(range(self.num_graphs)):\n",
    "#             node_features = torch.tensor(self.expr_mat.iloc[:,i].values)\n",
    "#             data = Data(x=node_features, edge_index=self.edge_index)\n",
    "#             data_list.append(data)\n",
    "#             torch.save(data, f\"./graphData/processed/{self.expr_mat.columns[i]}-graph.pt\")\n",
    "\n",
    "#         data, slices = self.collate(data_list)\n",
    "#         torch.save((data, slices), f\"./graphData/processed/combined.pt\")\n",
    "\n",
    "#     def get(self, idx=None, sample=None):\n",
    "#         if idx is None:\n",
    "#             data = torch.load(f\"./graphData/processed/{sample}-graph.pt\", weights_only=False)\n",
    "#         else:\n",
    "#             data = torch.load(f\"./graphData/processed/{self.expr_mat.columns[idx]}-graph.pt\", weights_only=False)\n",
    "#         return data\n",
    "\n",
    "\n",
    "# dataset = GeneGraphDataset(root='./graphData', edge_index=edges.t().contiguous(), expr_mat=metacellsReduced, meta=metadataReduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "281c0507-60d4-4df5-9784-e9ff17702db4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[141]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m num_samples = \u001b[38;5;28mmin\u001b[39m(\u001b[32m100\u001b[39m, metacellsReduced.shape[\u001b[32m1\u001b[39m], metadataReduced.shape[\u001b[32m0\u001b[39m])  \u001b[38;5;66;03m# Ensure we don't exceed available data\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[32m      8\u001b[39m     dataset.append(\n\u001b[32m      9\u001b[39m     Data(\n\u001b[32m     10\u001b[39m         x=torch.tensor(metacellsReduced.iloc[:, i].values, dtype=torch.float32).reshape((nGenes, \u001b[32m1\u001b[39m)),\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         edge_index=\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m.t().contiguous(),\n\u001b[32m     12\u001b[39m         y=torch.tensor(metadataReduced[\u001b[33m\"\u001b[39m\u001b[33mdisorder_encoded\u001b[39m\u001b[33m\"\u001b[39m].iloc[i], dtype=torch.long)\n\u001b[32m     13\u001b[39m     )\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m random.shuffle(dataset)\n\u001b[32m     16\u001b[39m loader = DataLoader(dataset, batch_size=\u001b[32m4\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "metadataReduced.loc[:,\"disorder_encoded\"] = le.fit_transform(metadataReduced[\"disorder\"])\n",
    "\n",
    "dataset = []\n",
    "nGenes = metacellsReduced.shape[0]\n",
    "num_samples = min(100, metacellsReduced.shape[1], metadataReduced.shape[0])  # Ensure we don't exceed available data\n",
    "for i in tqdm(range(num_samples)):\n",
    "    dataset.append(\n",
    "    Data(\n",
    "        x=torch.tensor(metacellsReduced.iloc[:, i].values, dtype=torch.float32).reshape((nGenes, 1)),\n",
    "        edge_index=torch.tensor(edges).t().contiguous(),\n",
    "        y=torch.tensor(metadataReduced[\"disorder_encoded\"].iloc[i], dtype=torch.long)\n",
    "    )\n",
    ")\n",
    "random.shuffle(dataset)\n",
    "loader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fa7c040f-dd62-4339-b578-da1036e0952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(metacells, metadata, edges, idx_train, idx_test, bs):\n",
    "    train = []\n",
    "    test = []\n",
    "    nGenes = metacells.shape[0]\n",
    "    for i in tqdm(idx_train):\n",
    "        train.append(Data(\n",
    "            x=torch.tensor(metacells.iloc[:,i].values, dtype=torch.float32).reshape((nGenes,1)),\n",
    "            edge_index=torch.tensor(edges).t().contiguous(),\n",
    "            y=torch.tensor(metadata[\"disorder_encoded\"].iloc[i], dtype=torch.long)\n",
    "        ))\n",
    "    for i in tqdm(idx_test):\n",
    "        test.append(Data(\n",
    "            x=torch.tensor(metacells.iloc[:,i].values, dtype=torch.float32).reshape((nGenes,1)),\n",
    "            edge_index=torch.tensor(edges).t().contiguous(),\n",
    "            y=torch.tensor(metadata[\"disorder_encoded\"].iloc[i], dtype=torch.long)\n",
    "        )) \n",
    "    random.shuffle(train)\n",
    "    random.shuffle(test)\n",
    "    return DataLoader(train, batch_size=bs), DataLoader(test, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "087ba992-e904-4591-a877-6666d2c7482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[:,\"disorder_encoded\"] = le.fit_transform(metadata[\"disorder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "97e58a07-9578-4459-bfdf-7f66396e9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_features = 1\n",
    "        self.hidden_layers = 1\n",
    "        self.k = 64\n",
    "        self.in_heads = 4\n",
    "        self.mid_heads = 2\n",
    "        self.out_heads = 1\n",
    "\n",
    "        self.conv1 = GATv2Conv(self.num_features, self.hidden_layers, heads=self.in_heads, dropout=0.2)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(self.hidden_layers * self.in_heads)\n",
    "        self.conv2 = GATv2Conv(self.hidden_layers*self.in_heads, self.hidden_layers, heads=self.mid_heads, dropout=0.2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(self.hidden_layers * self.mid_heads)\n",
    "        self.conv3 = GATv2Conv(self.hidden_layers*self.mid_heads, self.hidden_layers, heads=self.out_heads, dropout=0.2)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(self.hidden_layers)\n",
    "\n",
    "        self.pool = DMoNPooling([self.hidden_layers, self.hidden_layers], k=self.k, dropout=0.2)\n",
    "\n",
    "        self.proj = torch.nn.Linear(self.hidden_layers, 1)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(self.k, 2)\n",
    "\n",
    "    def _dense_adj(self, edge_index, alpha, batch):\n",
    "        # alpha = alpha.mean(dim=1)\n",
    "        # adj = to_dense_adj(edge_index, batch=batch, edge_attr=alpha)\n",
    "        # out = []\n",
    "        # for i in range(adj.size(0)):\n",
    "        #     A = adj[i]\n",
    "        #     deg_inv = A.sum(-1).clamp(min=1e-12).pow(-0.5)\n",
    "        #     norm_adj = deg_inv.unsqueeze(1) * A * deg_inv.unsqueeze(0)\n",
    "        #     out.append(norm_adj)\n",
    "        # return torch.stack(out)\n",
    "        alpha = alpha.mean(dim=1)\n",
    "        adj = to_dense_adj(edge_index, batch=batch, edge_attr=alpha)  # shape: [B, N, N]\n",
    "        deg = adj.sum(-1).clamp(min=1e-12)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm_adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n",
    "        return norm_adj\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, ei, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.elu(self.conv1(x, ei))\n",
    "        x = F.elu(self.conv2(x, ei))                      \n",
    "        x, (ei3, alpha3) = self.conv3(x, ei, return_attention_weights=True) \n",
    "        x = F.elu(self.bn3(x))\n",
    "\n",
    "        # convert to dense batch and corresponding mask\n",
    "        x_dense, mask = to_dense_batch(x, batch)\n",
    "\n",
    "        alpha = alpha3.mean(dim=1)\n",
    "        adj_dense = to_dense_adj(ei3, batch=batch, edge_attr=alpha)\n",
    "        deg = adj_dense.sum(-1).clamp(min=1e-12)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        adj_dense = deg_inv_sqrt.unsqueeze(-1) * adj_dense * deg_inv_sqrt.unsqueeze(-2)\n",
    "\n",
    "        # # build dense Laplacian surrogate from layer‑3 attention\n",
    "        # adj = self._dense_adj(ei3, alpha3, batch)\n",
    "        # mask = torch.ones(adj.size(0), adj.size(1), dtype=torch.bool, device=adj.device)\n",
    "\n",
    "        S, x, adj, mod, ort, clu = self.pool(x_dense, adj_dense, mask)\n",
    "\n",
    "        # read‑out\n",
    "        x = self.proj(x).squeeze(-1)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        pool_reg = mod + clu + 0.1 * ort\n",
    "        return logits, pool_reg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6bc6f624-5d77-4604-a8bd-9fe5bd19c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    iter_loss = []\n",
    "    train_iter = tqdm(loader)\n",
    "    correct_readouts = []\n",
    "    for data in train_iter:\n",
    "        data = data.to(device)\n",
    "        # data.y = data.y.long()\n",
    "        optimizer.zero_grad()\n",
    "        out, pool_reg = model(data)\n",
    "        loss = F.cross_entropy(out, data.y.squeeze()) + pool_reg.mean()\n",
    "        print(out)\n",
    "        pred = out.argmax(dim=1)\n",
    "        \n",
    "        correct_readout = [\"SCZ\" if item == 1 else \"CON\" for item in data.y]\n",
    "        for i, item in enumerate(pred == data.y):\n",
    "            correct_readout[i] += \"(√)\" if item else \"(x)\"\n",
    "        correct_readouts.extend(correct_readout)\n",
    "        train_iter.set_description(f\"(loss {loss.item()}; {(pred == data.y).sum().item()}/{loader.batch_size} [{\" \".join(correct_readout)}])\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train_iter.set_description(f\"(loss {loss.item()})\")\n",
    "        iter_loss.append(loss.item())\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    print(iter_loss)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def test_epoch(loader, model, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    correct_readouts = []\n",
    "    test_iter = tqdm(loader)\n",
    "    for data in test_iter:\n",
    "        data = data.to(device)\n",
    "        # data.y = data.y.long()\n",
    "        out, pool_reg = model(data)\n",
    "        print(out)\n",
    "        loss = F.cross_entropy(out, data.y.squeeze()) + pool_reg.mean()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        correct_readout = [\"SCZ\" if item == 1 else \"CON\" for item in data.y]\n",
    "        for i, item in enumerate(pred == data.y):\n",
    "            correct_readout[i] += \"(√)\" if item else \"(x)\"\n",
    "        correct_readouts.extend(correct_readout)\n",
    "        test_iter.set_description(f\"(loss {loss.item()}; {(pred == data.y).sum().item()}/{loader.batch_size} [{\" \".join(correct_readout)}])\")\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    print(correct_readouts)\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6e7847a-eec2-4cca-b58a-0c9a6cfd3412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:26<00:00,  2.67s/it]\n",
      "100%|██████████| 20/20 [00:53<00:00,  2.67s/it]\n",
      "(loss 7.755601406097412; 4/8 [SCZ(√) CON(x) SCZ(√) SCZ(√) CON(x) CON(x) SCZ(√) CON(x)]):   0%|          | 0/13 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.3350,  7.8380],\n",
      "        [-8.1754,  7.5557],\n",
      "        [-7.6658,  8.3093],\n",
      "        [-7.9162,  7.3422],\n",
      "        [-8.5165,  8.6737],\n",
      "        [-4.6252,  5.1736],\n",
      "        [-5.1895,  5.2223],\n",
      "        [-8.6381,  9.0780]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 5.965826511383057; 5/8 [SCZ(√) SCZ(√) CON(x) CON(x) SCZ(√) CON(x) SCZ(√) SCZ(√)]):   8%|▊         | 1/13 [00:28<03:55, 19.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7563,  2.4700],\n",
      "        [-7.5753,  7.3161],\n",
      "        [-9.5765, 10.3384],\n",
      "        [-4.3211,  5.1288],\n",
      "        [-9.6896, 10.3454],\n",
      "        [-8.6414,  8.1325],\n",
      "        [-3.9907,  3.3896],\n",
      "        [-7.3901,  7.3329]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 1.406540870666504; 7/8 [SCZ(√) SCZ(√) SCZ(√) CON(x) SCZ(√) SCZ(√) SCZ(√) SCZ(√)]):  15%|█▌        | 2/13 [00:47<03:35, 19.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.5134,  8.3789],\n",
      "        [-3.2028,  2.2591],\n",
      "        [-1.9902,  1.4948],\n",
      "        [-4.8717,  4.6920],\n",
      "        [-5.3413,  5.4917],\n",
      "        [-2.2897,  0.9355],\n",
      "        [-3.5557,  2.7508],\n",
      "        [-4.1006,  3.7156]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 6.334936141967773; 3/8 [SCZ(√) CON(x) CON(x) SCZ(√) SCZ(√) CON(x) CON(x) CON(x)]):  23%|██▎       | 3/13 [01:05<03:11, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9975,  1.8896],\n",
      "        [-2.2335,  1.3377],\n",
      "        [-3.1560,  5.0133],\n",
      "        [-2.8360,  3.8423],\n",
      "        [-3.2403,  6.2218],\n",
      "        [-6.0087,  6.6277],\n",
      "        [-5.9726,  7.2975],\n",
      "        [-4.7863,  6.6047]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 5.068115711212158; 3/8 [CON(x) SCZ(√) CON(x) SCZ(√) CON(x) CON(x) CON(x) SCZ(√)]):  31%|███       | 4/13 [01:25<02:51, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.3908,  4.9766],\n",
      "        [-4.0711,  6.6481],\n",
      "        [-3.6122,  3.2154],\n",
      "        [-2.4490,  3.3109],\n",
      "        [-2.9604,  3.1101],\n",
      "        [-4.6331,  5.4177],\n",
      "        [-3.6830,  2.9533],\n",
      "        [-2.7056,  2.6399]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 3.068268299102783; 3/8 [CON(x) CON(x) SCZ(x) CON(x) SCZ(√) CON(x) SCZ(√) SCZ(√)]):  38%|███▊      | 5/13 [01:44<02:32, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1328,  1.9511],\n",
      "        [-1.9524,  1.7715],\n",
      "        [-0.1078, -0.1396],\n",
      "        [-5.9280,  7.1430],\n",
      "        [-2.7101,  3.7611],\n",
      "        [-1.1157,  0.9273],\n",
      "        [-3.8785,  3.9066],\n",
      "        [-0.5194,  1.4269]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 1.3569869995117188; 7/8 [CON(√) SCZ(√) CON(√) CON(√) SCZ(√) SCZ(√) SCZ(√) CON(x)]):  46%|████▌     | 6/13 [02:03<02:13, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8878, -2.1236],\n",
      "        [-1.1889,  1.2264],\n",
      "        [ 1.4898, -0.8963],\n",
      "        [ 1.3275, -1.8695],\n",
      "        [-2.5372,  3.6928],\n",
      "        [-2.7458,  4.1521],\n",
      "        [-0.5886, -0.0566],\n",
      "        [-3.5248,  5.0081]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 1.8510932922363281; 5/8 [CON(x) CON(√) SCZ(√) CON(√) CON(x) SCZ(x) CON(√) SCZ(√)]):  54%|█████▍    | 7/13 [02:22<01:55, 19.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5166,  2.3181],\n",
      "        [-0.0971, -0.2287],\n",
      "        [-2.1619,  2.5948],\n",
      "        [ 0.6362, -1.1685],\n",
      "        [-0.7364,  1.0006],\n",
      "        [ 3.1607, -3.2469],\n",
      "        [ 1.0557, -1.3973],\n",
      "        [-1.1923,  0.4189]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 1.639204978942871; 5/8 [SCZ(√) CON(√) CON(x) CON(√) CON(x) CON(√) CON(√) CON(x)]):  62%|██████▏   | 8/13 [02:41<01:35, 19.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1452,  1.9779],\n",
      "        [ 0.5176, -0.6947],\n",
      "        [-1.8562,  2.6809],\n",
      "        [-0.0320, -0.1834],\n",
      "        [-2.2266,  2.5920],\n",
      "        [ 2.3947, -2.4158],\n",
      "        [ 0.8213, -1.0809],\n",
      "        [ 0.7137,  1.4583]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 3.2221479415893555; 3/8 [SCZ(√) SCZ(√) CON(x) SCZ(x) CON(x) SCZ(√) CON(x) SCZ(x)]):  69%|██████▉   | 9/13 [03:01<01:17, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6790,  2.0018],\n",
      "        [-0.5998,  0.8099],\n",
      "        [-1.3778,  1.4988],\n",
      "        [ 4.3874, -4.2341],\n",
      "        [-2.6363,  2.7654],\n",
      "        [-1.6076,  4.0961],\n",
      "        [-0.5761,  1.3819],\n",
      "        [ 2.3507, -2.5429]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 2.5492827892303467; 4/8 [CON(√) SCZ(√) SCZ(x) CON(x) SCZ(x) SCZ(√) SCZ(x) SCZ(√)]):  77%|███████▋  | 10/13 [03:21<00:58, 19.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6244, -0.3667],\n",
      "        [-1.9587,  1.0724],\n",
      "        [ 2.0684, -1.5145],\n",
      "        [-1.8887,  2.9309],\n",
      "        [ 2.7886, -1.2110],\n",
      "        [-1.6545,  2.1598],\n",
      "        [ 2.6585, -2.6945],\n",
      "        [-0.7705, -0.6245]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 4.637099742889404; 3/8 [SCZ(x) SCZ(x) CON(x) SCZ(√) CON(√) CON(√) CON(x) SCZ(x)]):  85%|████████▍ | 11/13 [03:40<00:38, 19.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.6644, -8.1291],\n",
      "        [ 1.3003, -0.2403],\n",
      "        [-2.4543,  3.9574],\n",
      "        [-1.6726, -0.7041],\n",
      "        [ 2.6408, -2.2582],\n",
      "        [ 2.2616, -1.5346],\n",
      "        [-1.5574,  2.3413],\n",
      "        [ 4.0824, -5.2320]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 0.31108415126800537; 4/8 [CON(√) SCZ(√) CON(√) CON(√)]):  92%|█████████▏| 12/13 [03:55<00:19, 19.31s/it]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4181, -1.1514],\n",
      "        [-2.6380,  2.6309],\n",
      "        [ 2.6778, -2.7521],\n",
      "        [ 0.2987, -0.4599]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 0.31108415126800537; 4/8 [CON(√) SCZ(√) CON(√) CON(√)]): 100%|██████████| 13/13 [04:01<00:00, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.755601406097412, 5.965826511383057, 1.406540870666504, 6.334936141967773, 5.068115711212158, 3.068268299102783, 1.3569869995117188, 1.8510932922363281, 1.639204978942871, 3.2221479415893555, 2.5492827892303467, 4.637099742889404, 0.31108415126800537]\n",
      "3.600851740837097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = GAT()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "device = 'cpu'\n",
    "\n",
    "#train_loader, test_loader = train_test(metacells, metadata, edges, list(range(0, 5000, 50)), list(range(25, 5025, 50)))\n",
    "#samples_use = random.sample(list(range(metacells.shape[1])), 300)\n",
    "con_samples = [metacells.columns.get_loc(c) for c in metadata[metadata[\"disorder_encoded\"] == 1].sample(n=120).index.values]\n",
    "scz_samples = [metacells.columns.get_loc(c) for c in metadata[metadata[\"disorder_encoded\"] == 0].sample(n=120).index.values]\n",
    "\n",
    "samples_train = con_samples[:100]\n",
    "samples_train.extend(scz_samples[:100])\n",
    "random.shuffle(samples_train)\n",
    "samples_test = con_samples[20:]\n",
    "samples_test.extend(scz_samples[20:])\n",
    "random.shuffle(samples_test)\n",
    "\n",
    "train_loader, test_loader = train_test(metacells, metadata, edges, samples_train, samples_test, 16)\n",
    "\n",
    "loss = train_epoch(train_loader, model, optimizer, device)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8624d042-6e1e-4eed-a271-89de5a5e6e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 4.835780143737793; 5/8 [CON(√) CON(√) CON(√) CON(√) SCZ(x) SCZ(x) SCZ(x) CON(√)]):  33%|███▎      | 1/3 [00:07<00:15,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.4445, -7.0102],\n",
      "        [ 6.6995, -7.3085],\n",
      "        [ 6.1920, -6.7347],\n",
      "        [ 7.2206, -7.8991],\n",
      "        [ 5.7347, -6.2378],\n",
      "        [ 4.9250, -5.3053],\n",
      "        [ 7.0807, -7.7250],\n",
      "        [ 4.3374, -4.6316]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 6.1008, -6.6298],\n",
      "        [ 8.5782, -9.4206],\n",
      "        [ 7.0398, -7.6904],\n",
      "        [ 1.5264, -1.4489],\n",
      "        [ 5.9499, -6.4575],\n",
      "        [ 5.5295, -5.9902],\n",
      "        [ 5.7168, -6.1994],\n",
      "        [ 7.0232, -7.6850]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 6.2702860832214355; 4/8 [SCZ(x) SCZ(x) SCZ(x) SCZ(x) CON(√) CON(√) CON(√) CON(√)]):  67%|██████▋   | 2/3 [00:16<00:08,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.5071, -9.3462],\n",
      "        [ 7.2600, -7.9316],\n",
      "        [ 6.6295, -7.2367],\n",
      "        [ 2.3972, -2.4397]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(loss 11.939884185791016; 1/8 [SCZ(x) SCZ(x) SCZ(x) CON(√)]): 100%|██████████| 3/3 [00:21<00:00,  7.14s/it]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CON(√)', 'CON(√)', 'CON(√)', 'CON(√)', 'SCZ(x)', 'SCZ(x)', 'SCZ(x)', 'CON(√)', 'SCZ(x)', 'SCZ(x)', 'SCZ(x)', 'SCZ(x)', 'CON(√)', 'CON(√)', 'CON(√)', 'CON(√)', 'SCZ(x)', 'SCZ(x)', 'SCZ(x)', 'CON(√)']\n"
     ]
    }
   ],
   "source": [
    "total_loss, correct = test_epoch(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa203185-2f3c-485e-80ed-f6fec3795d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
